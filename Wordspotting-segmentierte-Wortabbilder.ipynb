{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class Segmentation(object):\n",
    "    @staticmethod\n",
    "    def segmentCut(image, document_text):\n",
    "        segmentFile=open(document_text, 'r')\n",
    "        \n",
    "        lines = segmentFile.readlines()\n",
    "        listOfWordPics=list()\n",
    "        \n",
    "        for line in lines:\n",
    "            left0, top0, right0, bottom0, content = line.split(\" \")\n",
    "            left=int(left0)\n",
    "            right=int(right0)\n",
    "            top=int(top0)\n",
    "            bottom=int(bottom0)\n",
    "            \n",
    "            word=image.crop((left, top, right, bottom))\n",
    "            listOfWordPics.append((word,content))\n",
    "        \n",
    "        return listOfWordPics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BagOfFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import-Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as Image\n",
    "\n",
    "# matplotlib.use('Qt5Agg')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Für den gesamten Datensatz:\n",
    "- Graustufen-Darstellung erzeugen\n",
    "- Auf Vektoren reduzieren (lokale Bild-Deskriptoren - SIFT)\n",
    "\n",
    "2. Visual Vocabulary finden \n",
    "- Clustern\n",
    "- Gewichten\n",
    "\n",
    "3. Quantisieren\n",
    "- Abbildung von Deskriptoren auf Centruiden\n",
    "- Wie oft kommt ein vis. Word aus dem vis. Voc in dem Bild vor\n",
    "-> Histogramm\n",
    "\n",
    "4. Distanzen zwischen den Histogrammen vergleichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "data": {
      "text/plain": "[(<PIL.Image.Image image mode=L size=139x66 at 0x17DB6CC4970>, '270\\n'),\n (<PIL.Image.Image image mode=L size=248x103 at 0x17DB6CC6230>, 'letters\\n'),\n (<PIL.Image.Image image mode=L size=252x80 at 0x17DB6CC5E10>, 'orders\\n'),\n (<PIL.Image.Image image mode=L size=207x80 at 0x17DB6CC5990>, 'and\\n'),\n (<PIL.Image.Image image mode=L size=551x98 at 0x17DB6CC4D30>,\n  'instructions\\n'),\n (<PIL.Image.Image image mode=L size=246x100 at 0x17DB6CC70A0>, 'october\\n'),\n (<PIL.Image.Image image mode=L size=146x74 at 0x17DB6CC6050>, '1755\\n'),\n (<PIL.Image.Image image mode=L size=159x123 at 0x17DB6CC4C40>, 'only\\n'),\n (<PIL.Image.Image image mode=L size=131x104 at 0x17DB6CC63E0>, 'for\\n'),\n (<PIL.Image.Image image mode=L size=143x87 at 0x17DB6CC4760>, 'the\\n'),\n (<PIL.Image.Image image mode=L size=327x104 at 0x17DB6CC57E0>, 'publick\\n'),\n (<PIL.Image.Image image mode=L size=156x67 at 0x17DB6CC5420>, 'use\\n'),\n (<PIL.Image.Image image mode=L size=251x64 at 0x17DB6CC5A80>, 'unless\\n'),\n (<PIL.Image.Image image mode=L size=100x75 at 0x17DB6CC5210>, 'by\\n'),\n (<PIL.Image.Image image mode=L size=344x83 at 0x17DB6CC7430>, 'particu\\n'),\n (<PIL.Image.Image image mode=L size=124x79 at 0x17DB6CC5DB0>, 'lar\\n'),\n (<PIL.Image.Image image mode=L size=215x63 at 0x17DB6CC5360>, 'orders\\n'),\n (<PIL.Image.Image image mode=L size=246x106 at 0x17DB6CC7010>, 'from\\n'),\n (<PIL.Image.Image image mode=L size=159x44 at 0x17DB6CC78B0>, 'me\\n'),\n (<PIL.Image.Image image mode=L size=195x90 at 0x17DB6CC7490>, 'you\\n'),\n (<PIL.Image.Image image mode=L size=150x36 at 0x17DB6CC72B0>, 'are\\n'),\n (<PIL.Image.Image image mode=L size=63x71 at 0x17DB6CC7580>, 'to\\n'),\n (<PIL.Image.Image image mode=L size=235x56 at 0x17DB6CC5FF0>, 'send\\n'),\n (<PIL.Image.Image image mode=L size=222x58 at 0x17DB6CC75E0>, 'down\\n'),\n (<PIL.Image.Image image mode=L size=79x35 at 0x17DB6CC58A0>, 'a\\n'),\n (<PIL.Image.Image image mode=L size=294x62 at 0x17DB6CC5240>, 'barrel\\n'),\n (<PIL.Image.Image image mode=L size=108x88 at 0x17DB6CC5660>, 'of\\n'),\n (<PIL.Image.Image image mode=L size=235x94 at 0x17DB6CC7C40>, 'flints\\n'),\n (<PIL.Image.Image image mode=L size=222x79 at 0x17DB6CC7EB0>, 'with\\n'),\n (<PIL.Image.Image image mode=L size=136x67 at 0x17DB6CC7A60>, 'the\\n'),\n (<PIL.Image.Image image mode=L size=204x62 at 0x17DB6CC7B20>, 'arms\\n'),\n (<PIL.Image.Image image mode=L size=83x50 at 0x17DB6CC5F30>, 'to\\n'),\n (<PIL.Image.Image image mode=L size=406x104 at 0x17DB6CC5BD0>,\n  'winchester\\n'),\n (<PIL.Image.Image image mode=L size=211x52 at 0x17DB6CC4430>, 'and\\n'),\n (<PIL.Image.Image image mode=L size=255x68 at 0x17DB6CC5000>, 'about\\n'),\n (<PIL.Image.Image image mode=L size=139x56 at 0x17DB6CC4E20>, 'two\\n'),\n (<PIL.Image.Image image mode=L size=368x90 at 0x17DB6CC73A0>, 'thousand\\n'),\n (<PIL.Image.Image image mode=L size=203x83 at 0x17DB6CC4790>, 'weight\\n'),\n (<PIL.Image.Image image mode=L size=106x111 at 0x17DB6CC59F0>, 'of\\n'),\n (<PIL.Image.Image image mode=L size=227x95 at 0x17DB6CC5780>, 'flour\\n'),\n (<PIL.Image.Image image mode=L size=119x91 at 0x17DB6CC73D0>, 'for\\n'),\n (<PIL.Image.Image image mode=L size=124x75 at 0x17DB6CC7940>, 'the\\n'),\n (<PIL.Image.Image image mode=L size=122x52 at 0x17DB6CC5C00>, 'two\\n'),\n (<PIL.Image.Image image mode=L size=400x94 at 0x17DB6CC5840>, 'companies\\n'),\n (<PIL.Image.Image image mode=L size=102x83 at 0x17DB6CC4A00>, 'of\\n'),\n (<PIL.Image.Image image mode=L size=320x79 at 0x17DB6CC6E00>, 'rangers\\n'),\n (<PIL.Image.Image image mode=L size=246x62 at 0x17DB6CC4460>, 'twelve\\n'),\n (<PIL.Image.Image image mode=L size=366x83 at 0x17DB6CC5F00>, 'hundred\\n'),\n (<PIL.Image.Image image mode=L size=108x92 at 0x17DB6CC4580>, 'of\\n'),\n (<PIL.Image.Image image mode=L size=262x71 at 0x17DB6CC5300>, 'which\\n'),\n (<PIL.Image.Image image mode=L size=78x63 at 0x17DB6CC59C0>, 'to\\n'),\n (<PIL.Image.Image image mode=L size=72x63 at 0x17DB6CC4340>, 'be\\n'),\n (<PIL.Image.Image image mode=L size=331x58 at 0x17DB6CC5150>, 'delivered\\n'),\n (<PIL.Image.Image image mode=L size=359x95 at 0x17DB4F0D7B0>, 'captain\\n'),\n (<PIL.Image.Image image mode=L size=194x106 at 0x17DB4F0D990>, 'ashby\\n'),\n (<PIL.Image.Image image mode=L size=146x35 at 0x17DB4F0D870>, 'and\\n'),\n (<PIL.Image.Image image mode=L size=390x102 at 0x17DB4F0D840>, 'company\\n'),\n (<PIL.Image.Image image mode=L size=102x48 at 0x17DB4F0D2A0>, 'at\\n'),\n (<PIL.Image.Image image mode=L size=143x68 at 0x17DB4F0D330>, 'the\\n'),\n (<PIL.Image.Image image mode=L size=468x71 at 0x17DB4F0D300>, 'plantation\\n'),\n (<PIL.Image.Image image mode=L size=115x108 at 0x17DB4F0D1E0>, 'of\\n'),\n (<PIL.Image.Image image mode=L size=315x75 at 0x17DB4F0D1B0>, 'charles\\n'),\n (<PIL.Image.Image image mode=L size=336x88 at 0x17DB4F0EB90>, 'sellars\\n'),\n (<PIL.Image.Image image mode=L size=119x66 at 0x17DB4F0E920>, 'the\\n'),\n (<PIL.Image.Image image mode=L size=110x83 at 0x17DB4F0D9C0>, 'rest\\n'),\n (<PIL.Image.Image image mode=L size=52x88 at 0x17DB4F0D930>, 'to\\n'),\n (<PIL.Image.Image image mode=L size=148x83 at 0x17DB4F0D960>, 'captain\\n'),\n (<PIL.Image.Image image mode=L size=139x75 at 0x17DB4F0DA80>, 'cockes\\n'),\n (<PIL.Image.Image image mode=L size=379x100 at 0x17DB4F0DAB0>, 'company\\n'),\n (<PIL.Image.Image image mode=L size=126x91 at 0x17DB4F0DA20>, 'at\\n'),\n (<PIL.Image.Image image mode=L size=395x75 at 0x17DB4F0DA50>, 'nicholas\\n'),\n (<PIL.Image.Image image mode=L size=338x104 at 0x17DB4F0DB70>, 'reasmers\\n'),\n (<PIL.Image.Image image mode=L size=362x90 at 0x17DB4F0DBA0>, 'october\\n'),\n (<PIL.Image.Image image mode=L size=115x63 at 0x17DB4F0DB10>, '26\\n'),\n (<PIL.Image.Image image mode=L size=187x90 at 0x17DB4F0DB40>, 'gw\\n'),\n (<PIL.Image.Image image mode=L size=163x115 at 0x17DB4F0DC60>, '28th\\n'),\n (<PIL.Image.Image image mode=L size=460x94 at 0x17DB4F0DC90>, 'winchester\\n'),\n (<PIL.Image.Image image mode=L size=363x83 at 0x17DB4F0DC00>, 'october\\n'),\n (<PIL.Image.Image image mode=L size=99x94 at 0x17DB4F0DC30>, '28\\n'),\n (<PIL.Image.Image image mode=L size=198x91 at 0x17DB4F0C5E0>, '1755\\n'),\n (<PIL.Image.Image image mode=L size=318x83 at 0x17DB4F0EA70>, 'parole\\n'),\n (<PIL.Image.Image image mode=L size=458x110 at 0x17DB4F0C3A0>, 'hampton\\n'),\n (<PIL.Image.Image image mode=L size=152x79 at 0x17DB4F0C4C0>, 'the\\n'),\n (<PIL.Image.Image image mode=L size=226x79 at 0x17DB4F0EA40>, 'officers\\n'),\n (<PIL.Image.Image image mode=L size=131x56 at 0x17DB4F0EB60>, 'who\\n'),\n (<PIL.Image.Image image mode=L size=178x35 at 0x17DB4F0D2D0>, 'came\\n'),\n (<PIL.Image.Image image mode=L size=206x48 at 0x17DB4F0D3F0>, 'down\\n'),\n (<PIL.Image.Image image mode=L size=202x116 at 0x17DB4F0D420>, 'from\\n'),\n (<PIL.Image.Image image mode=L size=183x83 at 0x17DB4F0D5A0>, 'fort\\n'),\n (<PIL.Image.Image image mode=L size=566x100 at 0x17DB4F0D570>,\n  'cumberland\\n'),\n (<PIL.Image.Image image mode=L size=215x104 at 0x17DB4F0C8B0>, 'with\\n'),\n (<PIL.Image.Image image mode=L size=307x66 at 0x17DB4F0C7C0>, 'colonel\\n'),\n (<PIL.Image.Image image mode=L size=444x106 at 0x17DB4F0CB80>,\n  'washington\\n'),\n (<PIL.Image.Image image mode=L size=108x31 at 0x17DB4F0D750>, 'are\\n'),\n (<PIL.Image.Image image mode=L size=527x100 at 0x17DB4F0D780>,\n  'immediately\\n'),\n (<PIL.Image.Image image mode=L size=84x63 at 0x17DB4F0D8A0>, 'to\\n'),\n (<PIL.Image.Image image mode=L size=80x54 at 0x17DB4F0E7A0>, 'go\\n'),\n (<PIL.Image.Image image mode=L size=270x56 at 0x17DB4F0E950>, 'recrui\\n'),\n (<PIL.Image.Image image mode=L size=163x114 at 0x17DB4F0E6E0>, 'ting\\n'),\n (<PIL.Image.Image image mode=L size=135x44 at 0x17DB4F0E6B0>, 'and\\n'),\n (<PIL.Image.Image image mode=L size=163x79 at 0x17DB4F0E590>, 'they\\n'),\n (<PIL.Image.Image image mode=L size=95x60 at 0x17DB4F0E680>, 'are\\n'),\n (<PIL.Image.Image image mode=L size=392x64 at 0x17DB4F0D6F0>, 'allowed\\n'),\n (<PIL.Image.Image image mode=L size=204x63 at 0x17DB4F0C700>, 'until\\n'),\n (<PIL.Image.Image image mode=L size=115x75 at 0x17DB4F0C670>, 'the\\n'),\n (<PIL.Image.Image image mode=L size=89x107 at 0x17DB4F0E740>, '1st\\n'),\n (<PIL.Image.Image image mode=L size=92x81 at 0x17DB4F0F2E0>, 'of\\n'),\n (<PIL.Image.Image image mode=L size=118x71 at 0x17DB4F0C760>, 'de\\n'),\n (<PIL.Image.Image image mode=L size=235x87 at 0x17DB4F0EEF0>, 'cember\\n'),\n (<PIL.Image.Image image mode=L size=107x50 at 0x17DB4F0E9B0>, 'at\\n'),\n (<PIL.Image.Image image mode=L size=259x75 at 0x17DB4F0D8D0>, 'which\\n'),\n (<PIL.Image.Image image mode=L size=183x62 at 0x17DB4F0DFC0>, 'time\\n'),\n (<PIL.Image.Image image mode=L size=80x91 at 0x17DB4F0DFF0>, 'if\\n'),\n (<PIL.Image.Image image mode=L size=167x107 at 0x17DB4F0E110>, 'they\\n'),\n (<PIL.Image.Image image mode=L size=87x54 at 0x17DB4F0E140>, 'do\\n'),\n (<PIL.Image.Image image mode=L size=171x50 at 0x17DB4F0E0B0>, 'not\\n'),\n (<PIL.Image.Image image mode=L size=431x88 at 0x17DB4F0E0E0>, 'punctually\\n'),\n (<PIL.Image.Image image mode=L size=242x71 at 0x17DB4F0E5C0>, 'appear\\n'),\n (<PIL.Image.Image image mode=L size=100x46 at 0x17DB4F0C4F0>, 'at\\n'),\n (<PIL.Image.Image image mode=L size=115x71 at 0x17DB4F0C430>, 'the\\n'),\n (<PIL.Image.Image image mode=L size=207x103 at 0x17DB4F0D7E0>, 'place\\n'),\n (<PIL.Image.Image image mode=L size=112x96 at 0x17DB4F0FBE0>, 'of\\n'),\n (<PIL.Image.Image image mode=L size=263x100 at 0x17DB4F0FC70>, 'rendez\\n'),\n (<PIL.Image.Image image mode=L size=163x39 at 0x17DB4F0F130>, 'vous\\n'),\n (<PIL.Image.Image image mode=L size=331x98 at 0x17DB4F0C6A0>, 'assigned\\n'),\n (<PIL.Image.Image image mode=L size=207x90 at 0x17DB4F0EEC0>, 'them\\n'),\n (<PIL.Image.Image image mode=L size=167x98 at 0x17DB4F0EF50>, 'they\\n'),\n (<PIL.Image.Image image mode=L size=167x56 at 0x17DB4F0F1C0>, 'will\\n'),\n (<PIL.Image.Image image mode=L size=84x63 at 0x17DB4F0C970>, 'be\\n'),\n (<PIL.Image.Image image mode=L size=160x83 at 0x17DB4F0C580>, 'tried\\n'),\n (<PIL.Image.Image image mode=L size=96x79 at 0x17DB4F0F970>, 'by\\n'),\n (<PIL.Image.Image image mode=L size=74x71 at 0x17DB4F0C9D0>, 'a\\n'),\n (<PIL.Image.Image image mode=L size=211x79 at 0x17DB4F0EF20>, 'court\\n'),\n (<PIL.Image.Image image mode=L size=339x96 at 0x17DB4F0D060>, 'martial\\n'),\n (<PIL.Image.Image image mode=L size=108x87 at 0x17DB4F0CFD0>, 'for\\n'),\n (<PIL.Image.Image image mode=L size=503x66 at 0x17DB4F0FC10>,\n  'disobedience\\n'),\n (<PIL.Image.Image image mode=L size=98x83 at 0x17DB4F0D120>, 'of\\n'),\n (<PIL.Image.Image image mode=L size=227x62 at 0x17DB4F0D150>, 'orders\\n'),\n (<PIL.Image.Image image mode=L size=183x104 at 0x17DB4F0D0C0>, 'they\\n'),\n (<PIL.Image.Image image mode=L size=111x34 at 0x17DB4F0D0F0>, 'are\\n'),\n (<PIL.Image.Image image mode=L size=75x60 at 0x17DB4F0D210>, 'to\\n'),\n (<PIL.Image.Image image mode=L size=182x47 at 0x17DB4F0D240>, 'wait\\n'),\n (<PIL.Image.Image image mode=L size=192x71 at 0x17DB4F0DD50>, 'upon\\n'),\n (<PIL.Image.Image image mode=L size=118x64 at 0x17DB4F0DD80>, 'the\\n'),\n (<PIL.Image.Image image mode=L size=143x75 at 0x17DB4F0DCF0>, 'aid\\n'),\n (<PIL.Image.Image image mode=L size=91x79 at 0x17DB4F0DD20>, 'de\\n'),\n (<PIL.Image.Image image mode=L size=207x70 at 0x17DB4F0DE40>, 'camp\\n'),\n (<PIL.Image.Image image mode=L size=98x56 at 0x17DB4F0DE70>, 'at\\n'),\n (<PIL.Image.Image image mode=L size=131x35 at 0x17DB4F0DDE0>, 'one\\n'),\n (<PIL.Image.Image image mode=L size=107x94 at 0x17DB4F0DE10>, 'of\\n'),\n (<PIL.Image.Image image mode=L size=126x79 at 0x17DB4F0DF30>, 'the\\n'),\n (<PIL.Image.Image image mode=L size=238x82 at 0x17DB4F0DF60>, 'clock\\n'),\n (<PIL.Image.Image image mode=L size=67x56 at 0x17DB4F0DED0>, 'to\\n'),\n (<PIL.Image.Image image mode=L size=235x44 at 0x17DB4F0DF00>, 'receive\\n'),\n (<PIL.Image.Image image mode=L size=170x67 at 0x17DB4F0E020>, 'their\\n'),\n (<PIL.Image.Image image mode=L size=215x56 at 0x17DB4F0E050>, 'recrui\\n'),\n (<PIL.Image.Image image mode=L size=132x82 at 0x17DB4F0E200>, 'ting\\n'),\n (<PIL.Image.Image image mode=L size=458x83 at 0x17DB4F0E230>,\n  'instructions\\n'),\n (<PIL.Image.Image image mode=L size=220x70 at 0x17DB4F0E1A0>, 'each\\n'),\n (<PIL.Image.Image image mode=L size=228x75 at 0x17DB4F0E1D0>, 'officer\\n'),\n (<PIL.Image.Image image mode=L size=279x75 at 0x17DB4F0E2F0>, 'present\\n'),\n (<PIL.Image.Image image mode=L size=56x71 at 0x17DB4F0E320>, 'to\\n'),\n (<PIL.Image.Image image mode=L size=127x63 at 0x17DB4F0E290>, 'give\\n'),\n (<PIL.Image.Image image mode=L size=91x34 at 0x17DB4F0E2C0>, 'in\\n'),\n (<PIL.Image.Image image mode=L size=67x58 at 0x17DB4F0E3E0>, 'a\\n'),\n (<PIL.Image.Image image mode=L size=311x83 at 0x17DB4F0E410>, 'return\\n'),\n (<PIL.Image.Image image mode=L size=543x75 at 0x17DB4F0E380>,\n  'immediately\\n'),\n (<PIL.Image.Image image mode=L size=83x71 at 0x17DB4F0E3B0>, 'of\\n'),\n (<PIL.Image.Image image mode=L size=126x67 at 0x17DB4F0E4D0>, 'the\\n'),\n (<PIL.Image.Image image mode=L size=263x54 at 0x17DB4F0E500>, 'number\\n'),\n (<PIL.Image.Image image mode=L size=84x79 at 0x17DB4F0E470>, 'of\\n'),\n (<PIL.Image.Image image mode=L size=214x44 at 0x17DB4F0E4A0>, 'men\\n'),\n (<PIL.Image.Image image mode=L size=115x75 at 0x17DB4F0C2B0>, 'he\\n'),\n (<PIL.Image.Image image mode=L size=150x75 at 0x17DB4F0C3D0>, 'has\\n'),\n (<PIL.Image.Image image mode=L size=304x71 at 0x17DB4F0EB30>, 'enlisted\\n'),\n (<PIL.Image.Image image mode=L size=167x64 at 0x17DB4F0EC50>, 'one\\n'),\n (<PIL.Image.Image image mode=L size=386x90 at 0x17DB4F0EAA0>, 'subaltern\\n'),\n (<PIL.Image.Image image mode=L size=108x35 at 0x17DB4F0EA10>, 'one\\n'),\n (<PIL.Image.Image image mode=L size=320x111 at 0x17DB4F0EC20>, 'sergeant\\n'),\n (<PIL.Image.Image image mode=L size=135x35 at 0x17DB4F0CAC0>, 'one\\n'),\n (<PIL.Image.Image image mode=L size=355x107 at 0x17DB4F0CC70>, 'corporal\\n'),\n (<PIL.Image.Image image mode=L size=115x35 at 0x17DB4F0FFD0>, 'one\\n'),\n (<PIL.Image.Image image mode=L size=387x87 at 0x17DB4F0F8B0>, 'drummer\\n'),\n (<PIL.Image.Image image mode=L size=159x66 at 0x17DB4F0FF40>, 'and\\n'),\n (<PIL.Image.Image image mode=L size=262x75 at 0x17DB4F0F3A0>, 'twenty\\n'),\n (<PIL.Image.Image image mode=L size=147x100 at 0x17DB4F0F0D0>, 'five\\n'),\n (<PIL.Image.Image image mode=L size=283x83 at 0x17DB4F0FFA0>, 'private\\n'),\n (<PIL.Image.Image image mode=L size=191x47 at 0x17DB4F0EDD0>, 'men\\n'),\n (<PIL.Image.Image image mode=L size=126x62 at 0x17DB4F0F850>, 'are\\n'),\n (<PIL.Image.Image image mode=L size=88x87 at 0x17DB4F0F040>, 'to\\n'),\n (<PIL.Image.Image image mode=L size=231x79 at 0x17DB4F0D390>, 'mount\\n'),\n (<PIL.Image.Image image mode=L size=303x75 at 0x17DB4F0D3C0>, 'guard\\n'),\n (<PIL.Image.Image image mode=L size=266x83 at 0x17DB4F0D4E0>, 'today\\n'),\n (<PIL.Image.Image image mode=L size=203x71 at 0x17DB4F0D510>, 'and\\n'),\n (<PIL.Image.Image image mode=L size=63x79 at 0x17DB4F0D600>, 'to\\n'),\n (<PIL.Image.Image image mode=L size=139x67 at 0x17DB4F0D5D0>, 'be\\n'),\n (<PIL.Image.Image image mode=L size=300x67 at 0x17DB4F0D4B0>, 'relieved\\n'),\n (<PIL.Image.Image image mode=L size=87x67 at 0x17DB4F0D480>, 'to\\n'),\n (<PIL.Image.Image image mode=L size=259x75 at 0x17DB4F0CB50>, 'morrow\\n'),\n (<PIL.Image.Image image mode=L size=111x59 at 0x17DB4F0CA30>, 'at\\n'),\n (<PIL.Image.Image image mode=L size=132x59 at 0x17DB4F0D6C0>, 'ten\\n'),\n (<PIL.Image.Image image mode=L size=307x71 at 0x17DB4F0C790>, 'oclock\\n'),\n (<PIL.Image.Image image mode=L size=115x70 at 0x17DB4F0CA60>, 'all\\n'),\n (<PIL.Image.Image image mode=L size=270x91 at 0x17DB4F0C7F0>, 'reports\\n'),\n (<PIL.Image.Image image mode=L size=148x79 at 0x17DB4F0C940>, 'and\\n'),\n (<PIL.Image.Image image mode=L size=314x83 at 0x17DB4F0D660>, 'returns\\n'),\n (<PIL.Image.Image image mode=L size=115x35 at 0x17DB4F0EB00>, 'are\\n'),\n (<PIL.Image.Image image mode=L size=56x42 at 0x17DB4F0D690>, 'to\\n'),\n (<PIL.Image.Image image mode=L size=115x67 at 0x17DB4E84FD0>, 'be\\n'),\n (<PIL.Image.Image image mode=L size=255x62 at 0x17DB4E86BF0>, 'made\\n'),\n (<PIL.Image.Image image mode=L size=63x50 at 0x17DB4E86C20>, 'to\\n'),\n (<PIL.Image.Image image mode=L size=119x71 at 0x17DB4E869B0>, 'the\\n'),\n (<PIL.Image.Image image mode=L size=131x50 at 0x17DB4E86980>, 'aid\\n'),\n (<PIL.Image.Image image mode=L size=78x54 at 0x17DB4E87580>, 'de\\n'),\n (<PIL.Image.Image image mode=L size=235x71 at 0x17DB4E86740>, 'camp\\n')]"
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common import features\n",
    "from scipy.cluster.vq import kmeans2\n",
    "documentSegmentation=os.path.join('GT', '2700270.gtp')\n",
    "documentImage=os.path.join('pages', '2700270.png')\n",
    "image=Image.open(documentImage)\n",
    "\n",
    "\n",
    "wordImageList=Segmentation.segmentCut(image, documentSegmentation)\n",
    "wordImageList"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert object to 'str' for 'filename'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [212]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#plt.figure()\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#wordImageList[5][0].show()\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m#document_image_filename = os.path.join('pages', wordnames[i])\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwordImageList\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m gray \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(img,cv2\u001B[38;5;241m.\u001B[39mCOLOR_RGB2GRAY)\n\u001B[0;32m      7\u001B[0m frames, desc \u001B[38;5;241m=\u001B[39m features\u001B[38;5;241m.\u001B[39mcompute_sift_descriptors(gray,step_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m15\u001B[39m,cell_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: Can't convert object to 'str' for 'filename'"
     ]
    }
   ],
   "source": [
    "#plt.figure()\n",
    "#wordImageList[5][0].show()\n",
    "\n",
    "#document_image_filename = os.path.join('pages', wordnames[i])\n",
    "img = cv2.imread(wordImageList[5][0])\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "frames, desc = features.compute_sift_descriptors(gray,step_size = 15,cell_size = 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from common import features\n",
    "from scipy.cluster.vq import kmeans2\n",
    "\n",
    "#init.\n",
    "wordnames=['2700270_for.png','2700270_only.png','2700270_the.png','2700270_you.png','2700270_the1.png']\n",
    "step_size = 15\n",
    "cell_size = 3\n",
    "n_centroids = 4\n",
    "histograms=[]\n",
    "\n",
    "# ?müssen wir wir erst wieder die visu. Words finden (scannen aller Bilder)\n",
    "#For all Images:\n",
    "#- Change Color scale to gray\n",
    "#- build SIFT representation\n",
    "for i in wordImageList:\n",
    "    document_image_filename = os.path.join('pages', wordnames[i])\n",
    "    img = cv2.imread(document_image_filename)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    #sift = cv2.SIFT_create()\n",
    "    #kp = sift.detect(gray,None)\n",
    "    #desc = sift.compute(gray,kp)\n",
    "\n",
    "    frames, desc = features.compute_sift_descriptors(gray,step_size = 15,cell_size = 3)\n",
    "\n",
    "    #Open CV nutzen \n",
    "\n",
    "    #clustering of SIFT descriptors\n",
    "    # ?sollten wir nicht villeicht lieber mit den Zentruiden arbeiten?\n",
    "    # Wurde schon auf 8 Hautrichtungen quantisiert?\n",
    "    centroid, labels = kmeans2(desc, n_centroids, iter=20, minit='points')\n",
    "\n",
    "    #Warum erstellen wir wieder ein Histogramm?\n",
    "    # Reicht es nicht aus mit den Zentruiden zu arbeiten?\n",
    "    hist = np.bincount(labels)\n",
    "    histograms.append(hist)\n",
    "\n",
    "\n",
    "\n",
    "train_labels=np.array([['for'],\n",
    "                       ['only'],\n",
    "                       ['the'],\n",
    "                       ['you']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test=np.array([[ 5, 22, 17, 16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from common.classification import KNNClassifier\n",
    "import scipy.spatial.distance as ds\n",
    "\n",
    "\n",
    "knn_classifier = KNNClassifier(k_neighbors=3, metric='euclidean')\n",
    "knn_classifier.estimate(train_samples=histograms[:3],train_labels=train_labels)\n",
    "knn_test_labels = knn_classifier.classify2(test_samples=test)\n",
    "knn_test_labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "def classify1k(train_samples, train_labels,test_samples, metric='euclidean'):\n",
    "    distances = ds.cdist(train_samples, test_samples, metric=metric)\n",
    "    test_labels_indices = np.argmin(distances, axis=0)\n",
    "    return train_labels[test_labels_indices]\n",
    "\n",
    "\n",
    "result=classify1k(train_samples=histograms[:3],train_labels=train_labels,test_samples=test,metric='euclidean')\n",
    "result\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib.patches import Circle, Rectangle\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "draw_descriptor_cells = True\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(im_arr, cmap=cm.get_cmap('Greys_r'))\n",
    "ax.autoscale(enable=False)\n",
    "colormap = cm.get_cmap('jet')\n",
    "desc_len = cell_size * 4\n",
    "for (x, y), label in zip(frames, labels):\n",
    "    color = colormap(label / float(n_centroids))\n",
    "    circle = Circle((x, y), radius=1, fc=color, ec=color, alpha=1)\n",
    "    rect = Rectangle((x - desc_len / 2, y - desc_len / 2),\n",
    "                     desc_len, desc_len, alpha=0.08, lw=1)\n",
    "    ax.add_patch(circle)\n",
    "    if draw_descriptor_cells:\n",
    "        for p_factor in [0.25, 0.5, 0.75]:\n",
    "            offset_dyn = desc_len * (0.5 - p_factor)\n",
    "            offset_stat = desc_len * 0.5\n",
    "            line_h = Line2D((x - offset_stat, x + offset_stat),\n",
    "                            (y - offset_dyn, y - offset_dyn), alpha=0.08, lw=1)\n",
    "            line_v = Line2D((x - offset_dyn, x - offset_dyn),\n",
    "                            (y - offset_stat, y + offset_stat), alpha=0.08, lw=1)\n",
    "            ax.add_line(line_h)\n",
    "            ax.add_line(line_v)\n",
    "    ax.add_patch(rect)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "hist = np.bincount(labels)\n",
    "hist\n",
    "\n",
    "\n",
    "y_values=hist\n",
    "x_pos = np.arange(len(y_values))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(x_pos, y_values, width=0.9, align='center', alpha=0.4,color=[colormap(i / float(n_centroids)) for i in x_pos])\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}